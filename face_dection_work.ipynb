{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing Imprints\n",
    "### Face Dectection for an effect of an fixed focus across imprints of an selph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution\n",
    "\n",
    "\n",
    "    Using face detection on idle imprint to estimate coordinates that are centered, these are coordinates use to establish a focus window for all imprints of that selph.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import time\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "from scipy.ndimage import zoom\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import moviepy.editor as movpy\n",
    "# !pip install moviepy --upgrade\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Media pipe face detection class\n",
    "mp_face_detection = mp.solutions.face_detection\n",
    "# drawing tools\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "                                                                                                                                                                                            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to reverse idle and append it the end of the sequence\n",
    "\n",
    "\n",
    "def reverseIdle(inputVidfile, outfile):\n",
    "        # videoCapture method of cv2 return video object\n",
    "\n",
    "    # Pass absolute address of video file\n",
    "    cap = cv2.VideoCapture(inputVidfile)\n",
    "    size = (int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)),\n",
    "    int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT)))\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    # read method of video object will return\n",
    "    # a tuple with 1st element denotes whether\n",
    "    # the frame was read successfully or not,\n",
    "    # 2nd element is the actual frame.\n",
    "    fourcc = cv2.VideoWriter_fourcc('m', 'p', '4', 'v')\n",
    "    writer = cv2.VideoWriter(outfile, fourcc, fps, size)\n",
    "    # Grab the current frame.\n",
    "    check , vid = cap.read()\n",
    "\n",
    "    # counter variable for\n",
    "    # counting frames\n",
    "    counter = 0\n",
    "\n",
    "    # Initialize the value\n",
    "    # of check variable\n",
    "    check = True\n",
    "\n",
    "    frame_list = []\n",
    "\n",
    "    # If reached the end of the video\n",
    "    # then we got False value of check.\n",
    "\n",
    "    # keep looping until we\n",
    "    # got False value of check.\n",
    "    while(check == True):\n",
    "\n",
    "        check , vid = cap.read()\n",
    "\n",
    "        # Add each frame in the list by\n",
    "        # using append method of the List\n",
    "        frame_list.append(vid)\n",
    "\n",
    "        # increment the counter by 1\n",
    "        counter += 1\n",
    "\n",
    "    # last value in the frame_list is None\n",
    "    # because when video reaches to the end\n",
    "    # then false value store in check variable\n",
    "    # and None value is store in vide variable.\n",
    "\n",
    "    # removing the last value from the\n",
    "    # frame_list by using pop method of List\n",
    "    frame_list.pop()\n",
    "\n",
    "    frame_list.reverse()\n",
    "\n",
    "    for frame in frame_list:\n",
    "        writer.write(frame)\n",
    "    cap.release()\n",
    "    writer.release()    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### select first and last frame  ✌\n",
    "### select crop size  ✌\n",
    "### Draw a  crop rec box center on the face ✌ ✔\n",
    "### crop that region ✌ ✔\n",
    "### write it to file✔\n",
    "### get audio from original file ✔\n",
    "### return build video file ✔\n",
    "\n",
    "Clip first and last 2 seconds of video\n",
    "Collect coordinates of facial bounding box across 4 seconds of video\n",
    "Find median coordinates of collected coordinates, and average size (W,H) of facial bounding box\n",
    "Use median coordinates and size of facial bounding box and place in center of cropped frame\n",
    "Export\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = 'idle.mp4'\n",
    "\n",
    "inputVidfile = './selph_input_vids/'+ file\n",
    "reverseIdle(inputVidfile, \"outfile.mp4\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detectAndCrop(inputPath):\n",
    "    cap = cv2.VideoCapture(inputPath)\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    size = (400, 600)\n",
    "    with mp_face_detection.FaceDetection(\n",
    "        model_selection=0, min_detection_confidence=0.5) as face_detection:\n",
    "\n",
    "        while cap.isOpened():\n",
    "\n",
    "            success, image = cap.read()\n",
    "            # If loading a video, use 'break' instead of 'continue'.\n",
    "            if not success:\n",
    "                cap.release()\n",
    "#                 writer.release()    \n",
    "#                 cv2.destroyAllWindows()\n",
    "                print(\"detect and  {}\".format(inputPath))\n",
    "                break\n",
    "            h, w, _ =  image.shape\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "            image.flags.writeable = False\n",
    "            results = face_detection.process(image)\n",
    "            image.flags.writeable = True\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "            if results.detections:\n",
    "                for detection in results.detections:\n",
    "                    box = detection.location_data.relative_bounding_box\n",
    "                    rect_start_point = mp_drawing._normalized_to_pixel_coordinates(box.xmin, box.ymin ,w, h)\n",
    "                    rect_end_point = mp_drawing._normalized_to_pixel_coordinates(box.xmin + box.width, box.ymin + + box.height,w, h)\n",
    "                    nosep = mp_face_detection.get_key_point(detection, mp_face_detection.FaceKeyPoint.NOSE_TIP)\n",
    "                    nosepx = mp_drawing._normalized_to_pixel_coordinates(nosep.x, nosep.y ,w, h)\n",
    "                    box_mid = int(rect_start_point[0] + 120)\n",
    "                    m= int(size[0]/2)\n",
    "                    mp = int(nosepx[0] - m)\n",
    "                    crop_start = (mp, 100)\n",
    "                    crop_end = (size[0]+ int(nosepx[0] - m), size[1])\n",
    "#                     crop = image[crop_start[0]: crop_end[1] + crop_start[0]  , crop_start[0] : crop_end[0] ]\n",
    "#                     writer.write(crop)    \n",
    "        cap.release()\n",
    "    return crop_start , crop_end\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def justCrop(inputPath, outputPath, crop_start, crop_end):\n",
    "    cap2 = cv2.VideoCapture(inputPath)\n",
    "    fps = cap2.get(cv2.CAP_PROP_FPS)\n",
    "    size = (400, 600)\n",
    "\n",
    "    fourcc = cv2.VideoWriter_fourcc('m', 'p', '4', 'v')\n",
    "    writer = cv2.VideoWriter(outputPath, fourcc, fps, size)\n",
    "    while cap2.isOpened():\n",
    "        \n",
    "        status, frame = cap2.read()\n",
    "    \n",
    "        if not status:\n",
    "            cap2.release()\n",
    "            writer.release() \n",
    "            print(\"new crop size\", crop.shape, crop_start)\n",
    "            print('cropping done')\n",
    "            break\n",
    "            \n",
    "        crop = frame[crop_start[0]: crop_end[1] + crop_start[0] , crop_start[0] : crop_end[0] ]\n",
    "        \n",
    "        writer.write(crop)\n",
    "    cap2.release()\n",
    "    writer.release() \n",
    "        \n",
    "#         crop = clipped_zoom(crop, 1.0)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Folders\n",
    "#### Input Imprints\n",
    "- ./selph_input_vids\n",
    "### for output imprints\n",
    "- ./"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing idle imprint Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## do detection and crop \n",
    "file_idle = 'idle.mp4'\n",
    "\n",
    "idleSeq = './selph_input_vids/'+ file_idle\n",
    "idle_end = 'end{}'.format(file_idle)\n",
    "idleOut = 'idleout.mp4'\n",
    "\n",
    "final_idle = 'final_idle.mp4'\n",
    "print(\"start detecting start frames\")\n",
    "\n",
    "# detection and returning croping coordinates for idle \n",
    "crop_start , crop_end = detectAndCrop(idleSeq)\n",
    "\n",
    "# cropping idle\n",
    "justCrop(idleSeq, idleOut,crop_start , crop_end)\n",
    "\n",
    "reverseIdle(idleOut, idle_end)\n",
    "\n",
    "\n",
    "# compiling\n",
    "clip1 = movpy.VideoFileClip(idleOut)\n",
    "clip2 = movpy.VideoFileClip(idle_end)\n",
    "# clip3 = movpy.VideoFileClip(endOut)\n",
    "final_clip = movpy.concatenate_videoclips([clip1,clip2])\n",
    "final_clip.write_videofile(final_idle)\n",
    "\n",
    "# removing temp files\n",
    "os.remove(idle_end)\n",
    "os.remove(idleOut)\n",
    "print(\"OK DONE\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imprint  with idle imprint coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "file_idle = 'idle.mp4'\n",
    "\n",
    "idleSeq = './selph_input_vids/'+ file_idle\n",
    "\n",
    "\n",
    "file = 'Howareyou.mp4'\n",
    "\n",
    "inputVidfile = './selph_input_vids/'+ file\n",
    "\n",
    "\n",
    "rebuildVidFile = 'rebuildv3{}'.format(file)\n",
    "audiofile = 'audiofile.mp3'\n",
    "startOut = 'startout.mp4'\n",
    "\n",
    "\n",
    "print(\"setting up clips\")\n",
    "\n",
    "clip = movpy.VideoFileClip(inputVidfile)\n",
    "\n",
    "\n",
    "clip.audio.write_audiofile(audiofile)\n",
    "\n",
    "print(\"start detecting start frames\")\n",
    "    \n",
    "crop_start , crop_end = detectAndCrop(idleSeq)\n",
    "\n",
    "justCrop(inputVidfile, startOut,crop_start , crop_end)\n",
    "\n",
    "\n",
    "\n",
    "clip1 = movpy.VideoFileClip(startOut)\n",
    "\n",
    "\n",
    "audioclip = movpy.AudioFileClip(audiofile)\n",
    "\n",
    "videoclip = clip1.set_audio(audioclip)\n",
    "videoclip.write_videofile(rebuildVidFile)\n",
    "\n",
    "\n",
    "\n",
    "os.remove(audiofile)\n",
    "os.remove(startOut)\n",
    "\n",
    "print(\"OK DONE\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Following cell are exploring solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "face_detection = mp_face_detection.FaceDetection(model_selection=0, min_detection_confidence=0.5)\n",
    "inputVidfile = './selph_input_vids/selph16.mp4'\n",
    "audiofile = \"selph1322.mp3\"\n",
    "outfile = 'trueselphnoaudio.mp4'\n",
    "rebuildVidFile = 'rebuildselphselph16v2.mp4'\n",
    "\n",
    "cap = cv2.VideoCapture(inputVidfile)\n",
    "\n",
    "print('getting audio..')\n",
    "\n",
    "\n",
    "\n",
    "my_clip = movpy.VideoFileClip(inputVidfile)\n",
    "\n",
    "\n",
    "my_clip.audio.write_audiofile(audiofile)\n",
    "print('getting audio done..')\n",
    "\n",
    "\n",
    "\n",
    "crop_start = None\n",
    "crop_end = None\n",
    "\n",
    "cropping = True\n",
    "\n",
    "crop_widow_width =  int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)) - 20 \n",
    "crop_widow_height =  int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT)) - 40\n",
    "\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "print(\"fps\", fps)\n",
    "                  \n",
    "# size = ( crop_widow_width, crop_widow_height )\n",
    "size = (400, 600)\n",
    "\n",
    "print(\"size\",size)\n",
    "capture_duration = 5\n",
    "\n",
    "\n",
    "\n",
    "fourcc = cv2.VideoWriter_fourcc('m', 'p', '4', 'v')\n",
    "writer = cv2.VideoWriter(outfile, fourcc, fps, size )\n",
    "\n",
    "# while detecting:\n",
    "start_time = time.time()\n",
    "\n",
    "    \n",
    "while int(time.time() - start_time) < capture_duration :\n",
    "    success, image = cap.read()\n",
    "    \n",
    "\n",
    "#     image = cv2.cvtColor(cv2.flip(image, 1), cv2.COLOR_BGR2RGB)\n",
    "    # To improve performance, optionally mark the image as not writeable to\n",
    "    # pass by reference.\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    image.flags.writeable = False\n",
    "\n",
    "    # Draw the face detection annotations on the image.\n",
    "    results = face_detection.process(image)\n",
    "    \n",
    "    image.flags.writeable = True\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "    \n",
    "    h, w, _ =  image.shape\n",
    "    if results.detections:\n",
    "        for detection in results.detections:\n",
    "            box = detection.location_data.relative_bounding_box\n",
    "            rect_start_point = mp_drawing._normalized_to_pixel_coordinates(box.xmin, box.ymin ,w, h)\n",
    "            rect_end_point = mp_drawing._normalized_to_pixel_coordinates(box.xmin + box.width, box.ymin + + box.height,w, h)\n",
    "            box_mid = int(rect_start_point[0] + 120)\n",
    "            \n",
    "            \n",
    "            nosep = mp_face_detection.get_key_point(detection, mp_face_detection.FaceKeyPoint.NOSE_TIP)\n",
    "            nosepx = mp_drawing._normalized_to_pixel_coordinates(nosep.x, nosep.y ,w, h)\n",
    "            \n",
    "            m= int(size[0]/2)\n",
    "            mp = int(nosepx[0] - m)\n",
    "            crop_start = (mp, nosepx[1] -  250)\n",
    "\n",
    "\n",
    "            crop_end = (size[0]+ int(nosepx[0] - m), size[1])\n",
    "\n",
    "\n",
    "\n",
    "#             m= int(size[0]/2)\n",
    "#             mp = int(nosepx[0] - m)\n",
    "#             crop_start = (mp, nosepx[1] -  250)\n",
    "\n",
    "#             cv2.circle(image, crop_start , 2, (255,0,0), 10)\n",
    "\n",
    "#             crop_end = (size[0]+ int(nosepx[0] - m), size[1])\n",
    "\n",
    "#             crop = image[crop_start[0]: crop_end[1] + crop_start[0] , crop_start[0] : crop_end[0] + crop_start[0]]\n",
    "    \n",
    "# detecting = False\n",
    "cap.release()\n",
    "print('detecting done')\n",
    "  \n",
    "print('START cropping ')\n",
    "cap2 = cv2.VideoCapture(inputVidfile)\n",
    "\n",
    "\n",
    "while cropping:\n",
    "    status, frame = cap2.read()\n",
    "    \n",
    "    if not status:\n",
    "        cap2.release()\n",
    "        writer.release() \n",
    "        cropping = False\n",
    "        print(\"new crop size\", crop.shape, crop_start)\n",
    "        print('cropping done')\n",
    "        break\n",
    "\n",
    "#     crop = frame[crop_start[1]: crop_end[0], crop_start[0]:  crop_end[1]]\n",
    "#     crop = frame[crop_start[0]: crop_end[1] +crop_start[0], crop_start[1] : crop_end[0]  + crop_start[1]]\n",
    "#     crop = frame[crop_start[0]: crop_end[1] + crop_start[0] , crop_start[0] : crop_end[0] + crop_start[0]]\n",
    "    crop = frame[crop_start[0]: crop_end[1] + crop_start[0] , crop_start[0] : crop_end[0] ]\n",
    "    crop = clipped_zoom(crop, 1.0)\n",
    "    \n",
    "    writer.write(crop)\n",
    "\n",
    "print(\"rebuilding....\")\n",
    "clip = movpy.VideoFileClip(outfile, audio=False)\n",
    "  \n",
    "# loading audio file\n",
    "audioclip = movpy.AudioFileClip(audiofile)\n",
    "\n",
    "\n",
    "videoclip = clip.set_audio(audioclip)\n",
    "videoclip.write_videofile(rebuildVidFile)\n",
    "print('removing temp files')\n",
    "os.remove(audiofile)\n",
    "os.remove(outfile)\n",
    "\n",
    "print(\"DONE....\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture('./selph_input_vids/Goodbye.mp4')\n",
    "\n",
    "crop_widow_width =  int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)) - 100\n",
    "crop_widow_height =  int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT)) - 50\n",
    "\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "\n",
    "                  \n",
    "size = (400, 600)\n",
    "# crop_size = (656, 520)\n",
    "\n",
    "fourcc = cv2.VideoWriter_fourcc('m', 'p', '4', 'v')\n",
    "writer = cv2.VideoWriter('newselph1333.mp4', fourcc, fps, size)\n",
    "# print(size)\n",
    "\n",
    "with mp_face_detection.FaceDetection(\n",
    "    model_selection=0, min_detection_confidence=0.5) as face_detection:\n",
    "    \n",
    "    while cap.isOpened():\n",
    "        \n",
    "        success, image = cap.read()\n",
    "        # If loading a video, use 'break' instead of 'continue'.\n",
    "        if not success:\n",
    "            cap.release()\n",
    "            writer.release()    \n",
    "            cv2.destroyAllWindows()\n",
    "            print(\"empty camera frame.\")\n",
    "            break\n",
    "        h, w, _ =  image.shape\n",
    "#         print(h,w)                 \n",
    "\n",
    "        # Flip the image horizontally for a later selfie-view display, and convert\n",
    "        # the BGR image to RGB.\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "#         image = cv2.cvtColor(cv2.flip(image, 1), cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # To improve performance, optionally mark the image as not writeable to\n",
    "        # pass by reference.\n",
    "        image.flags.writeable = False\n",
    "        results = face_detection.process(image)\n",
    "\n",
    "        # Draw the face detection annotations on the image.\n",
    "        image.flags.writeable = True\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "        \n",
    "        if results.detections:\n",
    "            for detection in results.detections:\n",
    "#                 mp_drawing.draw_detection(image, detection)\n",
    "                \n",
    "\n",
    "                box = detection.location_data.relative_bounding_box\n",
    "                rect_start_point = mp_drawing._normalized_to_pixel_coordinates(box.xmin, box.ymin ,w, h)\n",
    "                rect_end_point = mp_drawing._normalized_to_pixel_coordinates(box.xmin + box.width, box.ymin + + box.height,w, h)\n",
    "         \n",
    "    \n",
    "                nosep = mp_face_detection.get_key_point(detection, mp_face_detection.FaceKeyPoint.NOSE_TIP)\n",
    "                nosepx = mp_drawing._normalized_to_pixel_coordinates(nosep.x, nosep.y ,w, h)\n",
    "\n",
    "                box_mid = int(rect_start_point[0] + 120)\n",
    "                x_start =  int(rect_start_point[0] - 100 )\n",
    "\n",
    "\n",
    "                y_start =  int(rect_start_point[1] - 200)\n",
    "                \n",
    "#                 cv2.circle(image, (box_mid, int(rect_start_point[1] +  (rect_start_point[1] / 2)) ) , 2, (255,0,0), 10)\n",
    "#                 cv2.line(image, (20,box_mid), (box_mid,box_mid),(255,0,0), 10)\n",
    "#                 cv2.line(image, (nosepx[0] - (nosepx[0] - 20) ,nosepx[1]), nosepx ,(255,0,150), 10)\n",
    "#                 cv2.line(image, nosepx, (crop_end[0] - 50, nosepx[1]),(255,200,0), 10)\n",
    "#                 cv2.line(image, nosepx, (nosepx[0]*2, nosepx[1]),(255,200,0), 10)\n",
    "    \n",
    "#                 crop_start = (int(rect_end_point[0]/2) - int(rect_end_point[0]/2) , rect_start_point[1] -  150)\n",
    "#                 crop_start =( rect_start_point[0] - (rect_start_point[0] - 10), rect_start_point[1] - 150)\n",
    "                m= int(size[0]/2)\n",
    "                mp = int(nosepx[0] - m)\n",
    "        \n",
    "                                \n",
    "                crop_start = (mp, 100)\n",
    "\n",
    "                cv2.circle(image, crop_start , 2, (255,0,0), 10)\n",
    "\n",
    "                crop_end = (size[0]+ int(nosepx[0] - m), size[1])\n",
    "#                 crop_end = (size[0], size[1])\n",
    "#                 cv2.circle(image, crop_end , 2, (255,0,0), 10)\n",
    "\n",
    "#                 cv2.rectangle(image, rect_start_point , rect_end_point , (255,255,255), 3)\n",
    "\n",
    "#                 cv2.rectangle(image, crop_start , crop_end , (255,255,255), 3)\n",
    "\n",
    "                \n",
    "\n",
    "\n",
    "#                 crop_start = (rect_start_point[0] -110, rect_start_point[1] -200)\n",
    "                crop = image[crop_start[0]: crop_end[1] + crop_start[0]  , crop_start[0] : crop_end[0] ]\n",
    "#                 crop = clipped_zoom(crop, 1.0)\n",
    "                writer.write(crop)    \n",
    "                print(mp,nosepx, rect_end_point, crop_start , crop_end, crop.shape)\n",
    "                cv2.rectangle(image, crop_start , crop_end , (255,255,255), 3)\n",
    "        \n",
    "                cv2.putText(\n",
    "                             image, #numpy array on which text is written\n",
    "                             \"{}\".format(str(nosepx)), #text\n",
    "                             (100,50), #position at which writing has to start\n",
    "                             cv2.FONT_HERSHEY_SIMPLEX, #font family\n",
    "                             1, #font size\n",
    "                             (209, 80, 0), #font color\n",
    "                             3)\n",
    "\n",
    "#                 cv2.imshow('crop 22', image)\n",
    "                cv2.imshow('crop', image)\n",
    "        if cv2.waitKey(5) & 0xFF == 27:\n",
    "          break\n",
    "    cap.release()\n",
    "    writer.release()    \n",
    "    cv2.destroyAllWindows()\n",
    "    # print \"Video stop\"\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "file = 'Howareyou.mp4'\n",
    "\n",
    "inputVidfile = './selph_input_vids/'+ file\n",
    "\n",
    "fourcc = cv2.VideoWriter_fourcc('m', 'p', '4', 'v')\n",
    "writer = cv2.VideoWriter('newselph1333.mp4', fourcc, fps, size)\n",
    "\n",
    "# videoCapture method of cv2 return video object\n",
    "\n",
    "# Pass absolute address of video file\n",
    "cap = cv2.VideoCapture(inputVidfile)\n",
    "\n",
    "# read method of video object will return\n",
    "# a tuple with 1st element denotes whether\n",
    "# the frame was read successfully or not,\n",
    "# 2nd element is the actual frame.\n",
    "\n",
    "# Grab the current frame.\n",
    "check , vid = cap.read()\n",
    "\n",
    "# counter variable for\n",
    "# counting frames\n",
    "counter = 0\n",
    "\n",
    "# Initialize the value\n",
    "# of check variable\n",
    "check = True\n",
    "\n",
    "frame_list = []\n",
    "\n",
    "# If reached the end of the video\n",
    "# then we got False value of check.\n",
    "\n",
    "# keep looping until we\n",
    "# got False value of check.\n",
    "while(check == True):\n",
    "\t\n",
    "\t# imwrite method of cv2 saves the\n",
    "\t# image to the specified format.\n",
    "\tcv2.imwrite(\"frame%d.jpg\" %counter , vid)\n",
    "\tcheck , vid = cap.read()\n",
    "\t\n",
    "\t# Add each frame in the list by\n",
    "\t# using append method of the List\n",
    "\tframe_list.append(vid)\n",
    "\t\n",
    "\t# increment the counter by 1\n",
    "\tcounter += 1\n",
    "\n",
    "# last value in the frame_list is None\n",
    "# because when video reaches to the end\n",
    "# then false value store in check variable\n",
    "# and None value is store in vide variable.\n",
    "\n",
    "# removing the last value from the\n",
    "# frame_list by using pop method of List\n",
    "frame_list.pop()\n",
    "\n",
    "# looping in the List of frames.\n",
    "for frame in frame_list:\n",
    "\t\n",
    "\t# show the frame.\n",
    "\tcv2.imshow(\"Frame\" , frame)\n",
    "\t\n",
    "\t# waitkey method to stoping the frame\n",
    "\t# for some time. q key is presses,\n",
    "\t# stop the loop\n",
    "\tif cv2.waitKey(25) and 0xFF == ord(\"q\"):\n",
    "\t\tbreak\n",
    "\t\n",
    "# release method of video\n",
    "# object clean the input video\n",
    "cap.release()\n",
    "\n",
    "# close any open windows\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "# reverse the order of the element\n",
    "# present in the list by using\n",
    "# reverse method of the List.\n",
    "frame_list.reverse()\n",
    "\n",
    "for frame in frame_list:\n",
    "\tcv2.imshow(\"Frame\" , frame)\n",
    "\tif cv2.waitKey(25) and 0xFF == ord(\"q\"):\n",
    "\t\tbreak\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
